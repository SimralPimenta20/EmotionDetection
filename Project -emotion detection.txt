FIRST YEAR PROJECT DOCUMENTATION


Emotion detection from a live feed
  
Introduction:
Machine emotional intelligence is a burgeoning frontier that could have huge consequences in not only advertising, but in new startups, healthcare, wearables, education, and more.


Emotive analytics is an interesting blend of psychology and technology. Though arguably reductive, many facial expression detection tools lump human emotion into 5 main categories: Happiness, Sadness,neutral, Anger,and Fear. With facial emotion detection, algorithms detect faces within a photo or video, and sense micro expressions by analyzing the relationship between points on the face, based on curated databases compiled in academic environments.


STEPWISE MAKING OF THE PROJECT:




STEP 1:


Installation of python and required python libraries 


1.Python :The entire project is written in Python version 3.5. The use of Python version 3 is recommended as tensorflow (and other neural networks don't work with versions lesser than that)
Download: https://www.python.org/downloads/release/python-350/
Video reference:https://youtu.be/YdNiifNwt_M


2.pip:pip is a package management system used to install and manage software packages written in Python.
Download:Open a command prompt window and navigate to the folder containing get-pip.py . Then run python get-pip.py . This will install pip . Verify a successful installation by opening a command prompt window and navigating to your Python installation's script directory


3.opencv:OpenCV is used for many Image and Video processing applications such as Facial Recognition, Object Detection, Photo Editing, Robotic Vision, Character Recognition and so much more!
Download: pip3 install opencv-python
Video reference: (first 3 tutorials)https://youtu.be/YY9f-6u2Q_c


4.numpy:NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.
Download: pip3 install numpy //in cmd


5.tensorflow:TensorFlow™ is an open source software library for high performance numerical computation. Its flexible architecture allows easy deployment of computation across a variety of platforms . Originally developed by researchers and engineers from the Google Brain team within Google’s AI organization, it comes with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domains.
Download:pip3 install tensorflow


6.keras: Keras is an open source neural network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, Theano, or MXNet.
Download: pip3 install keras


7.sklearn: Scikit-learn is a free software machine learning library for the Python programming language.It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy
Download: pip3 install sklearn 


8.dlib:From a computer vision perspective, dlib has a number of state-of-the-art implementations, including:
a.Facial landmark detection
b.Correlation tracking
c.Deep metric learning
More about dlib: https://www.pyimagesearch.com/2017/03/27/how-to-install-dlib
Download link:https://pypi.org/simple/dlib/.
 
9.Pygame : pygame is a cross-platform set of Python modules designed for writing video games. It includes computer graphics and sound libraries designed to be used with the Python programming language.
We used it in our application to play appropriate music which was in ogg file format
Download: pip3 install pygame


11.A site that could help in installation of different python libraries :
Link: https://www.lfd.uci.edu/~gohlke/pythonlibs/


STEP 2:


Detection of facial landmarks and extraction of 68 relative distances
There were several options we encountered while deciding the form in which input to the neural network was to be provided
1. Detecting the face from the screen and feeding pixels
2. Detecting and extracting facial features like nose , eyebrows,lips etc and feeding that information
3. Detecting 68 coordinates of the face from the screen and feeding the coordinate values
4. Taking the 3rd method and making it a little more accurate by calculating the centre of gravity and removing the relative distances of the 68 coordinates wrt the centre of gravity-This is exactly what we did.
Procedure:
* Apply a pre-trained HOG + Linear SVM object detector specifically for the task of face detection.
* The below written code includes a function made called “get-landmarks” which is used in the training code automatically each time for training and testing of data. It uses the hog face detector for extraction of 68 key  points from the face feeded to it and calculates the mean of those 68 (x,y) coordinates that make the centre of gravity. After removing the cog it calculates the relative distances of those 68 points with respect to the cog and returns those 68 values in a list.
* The following technique is the best input method as it eliminates the following conditions that could cause trouble-
   1. The face being on a particular side on the webcam and hence giving different coordinate values.
   2. A tilted face giving different coordinate values.
In both cases relative distances about cog won't change and the trouble could be avoided and hence this is the best method.


Code of getlandmarks:
import cv2
import numpy as np
import dlib
import math
detector = dlib.get_frontal_face_detector()


predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks (1).dat")




def get_landmarks(image):
    detections = detector(image, 1)
    for k,d in enumerate(detections): #For all detected face instances individually
        shape = predictor(image, d) #Draw Facial Landmarks with the predictor class
        xlist = []
        ylist = []
        for i in range(0,68): #Store X and Y coordinates in two lists
            xlist.append(float(shape.part(i).x))
            ylist.append(float(shape.part(i).y))
            
        xmean = np.mean(xlist) #Find both coordinates of centre of gravity
        ymean = np.mean(ylist)
        #xcentral = [(x-xmean) for x in xlist] #Calculate distance centre <-> other points in both axes
        #ycentral = [(y-ymean) for y in ylist]
        reldis = []
        for i in range(0,68):
                reldis.append(math.sqrt((xmean-xlist[i])**2+(ymean-ylist[i])**2))
        
        return reldis






STEP 3:


Making of the dataset and the corresponding CSV file
Neural networks are data hungry. One of the hardest problems to solve in deep learning has nothing to do with neural nets: it’s the problem of getting the right data in the right format.

Getting the right data means gathering or identifying the data that correlates with the outcomes you want to predict; i.e. data that contains a signal about events you care about. The data needs to be aligned with the problem you’re trying to solve. Kitten pictures are not very useful when you’re building a facial identification system. Verifying that the data is aligned with the problem you seek to solve must be done by a data scientist. If you do not have the right data, then your efforts to build an AI solution must return to the data collection stage.


1. The first set you use is the training set.Running a training set through a neural network teaches the net how to weigh different features, adjusting them coefficients according to their likelihood of minimizing errors in your results.Those coefficients, also known as parameters, will be contained in tensors and together they are called the model, because they encode a model of the data they train on. They are the most important takeaways you will obtain from training a neural network. 
2. The second set is your test set. It functions as a seal of approval, and you don’t use it until the end. After you’ve trained and optimized your data, you test your neural net against this final random sampling. The results it produces should validate that your net accurately recognizes images, or recognizes them at least [x] percentage of them.
3. The dataset made in the corresponding project was divided in a ratio of 80:20 where 80% of it was used in training the NN and the rest 20% for testing.
4. As said before NN are data hungry. The more the data greater the accuracy of the program.Hence the dataset was kept on increasing until we achieved a desired accuracy.
5. The dataset we used for emotion recognition was a self-made dataset. We did try already made datasets available on the internet which were not satisfactory due to not being descriptive enough for training the neural network properly.
6. About 1000 images were downloaded for the mentioned 5 emotions and the CSV excel file was made with the name of the images in the first column and the corresponding emotion in the second.
7. However our self made dataset didn't work as well at first as we thought and faced a certain error which we had to filter out. 
   * To remove the non-cv2-supported files
   * To grayscale all images because it makes dilb face landmarks detection easier to work on the images
   * To resize all images to a specific pixel  size (400×400 here!)
   * There are certain images where the 68 points couldn't be detected . To remove such images and make a new CSV file that had images that were perfectly ready to train
After making data set and corresponding csv file :
1st code : to remove non-cv2-supported files and grayscaling along with resizing!
import os
import cv2
import pandas as pd 


path1 ="pics"
path2 = "gray pics"
names_of_images = os.listdir(path1)


c=0
for file in names_of_images:
    im = cv2.imread(path1+'//'+file,1)
    try:
        img = cv2.resize(im,(400,400))
    except:
        print(file)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    cv2.imwrite(path2 +'//'+file,gray)
2nd code:to remove images that do not give 68 distances and make a new csv file with the ones that give the 68 distances we use the following code!
from numpy import array
from reldist import get_landmarks
from PIL  import Image
import pandas as pd 
import csv
import cv2


path1 = "gray pics"


with open('1.csv','rt') as inp,open('2.csv','wt') as out:
    writer = csv.writer(out)    
    for row in csv.reader(inp):
        m=(array([array(Image.open(path1+'//'+row[1]))]))
        for n in m:
            if get_landmarks(n) != None:
                writer.writerow(row)
3.in the new csv file we follow the given steps:
* Go to "Find and Select " and select Go to special..
* Then mark the Blanks radio button
* Then Ctrl +' - ' + ' - '
* select Shift cells up radio button. 
4.then to remove remaining none type images that give an error by generating their row numbers with the following code ,we delete these rows manually!
import os
import pandas as pd
from reldist import get_landmarks
from PIL import Image
from numpy import array


data = pd.read_csv("2.csv")
path1 = "gray pics"


c = 0
imlist = data.iloc[0:277,1]




X_train1 = array([array(Image.open(path1+'//'+a))for a in imlist])
for file in X_train1:
    c=c+1
    try:
        b = len(get_landmarks(file))
    except:
        print (c)
        continue
5.after deleting the rows the csv file is ready.




STEP 4: 
Training and testing of the self-made dataset in ratio-80:20 
Code:
from reldist import get_landmarks
from keras.preprocessing.image import load_img,array_to_img
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense,Flatten
from keras.layers import Conv2D,MaxPooling2D
from keras.models import model_from_json
import json




import numpy as np
import matplotlib.pyplot as plt
import os
from PIL import Image
from numpy import array
import sys
import pandas as pd
from sklearn import preprocessing
sys.__stdout__ = sys.stdout


data = pd.read_csv("2.csv")
path1 = "gray pics"
training_data = data.iloc[0:222,1] #80% for training
training_ans = data.iloc[0:222,2]
test_data = data.iloc[223:277,1] #20% for testing
test_ans = data.iloc[223:277,2]
imlist = training_data
imlist1 = test_data
le = preprocessing.LabelEncoder()
le.fit(training_ans)
le1 = preprocessing.LabelEncoder()
le1.fit(test_ans)


X_train1 = array([array(Image.open(path1+'//'+a))
                  for a in imlist])


X_train2 = []


for file in X_train1:
    #try:
        #print (len(get_landmarks(file)))
    X_train2.append(get_landmarks(file))
    #except:
        #continue
    


y_train =le.transform(training_ans)


X_test1 = array([array(Image.open(path1  + '//' + b))
                 for b in imlist1])


X_test2 = []


for file in X_test1:
    X_test2.append(get_landmarks(file))
    
y_test = le1.transform(test_ans)
X_train = np.array(X_train2)
X_test = np.array(X_test2)
y_test = to_categorical(y_test,5)
y_train = to_categorical(y_train,5)


model = Sequential()
model.add(Dense(1080,activation = 'sigmoid',input_shape=(68,)))
model.add(Dense(1080,activation = 'sigmoid'))
model.add (Dense(5,activation = 'softmax'))
model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'])
model.summary()
#c = 0
#try:
history = model.fit(X_train,y_train,epochs=20,validation_data = (X_test,y_test))
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.plot(history.history['loss'])


s_json =model.to_json()
with open("s.json","w") as json_file:
        json_file.write(s_json)
model.save_weights("s.h5")




STEP 5: 
Using the dataset for emotion recognition.
After training the dataset and saving it as a JSON model we need to use it for emotion recognition from live feed which is our final goal.                                  The following program will use the already trained and saved JSON model, take live feed of the person from the webcam and print percentage of all emotions detected.
Code:
from reldist import get_landmarks
from keras.models import model_from_json
from app import happy,anger,neutral,sad,fear


import json
import cv2
import dlib
import numpy as np


camera = cv2.VideoCapture(0)
a=[]
n = 10
sumanger = 0
sumfear = 0
sumhappy = 0
sumneutral = 0
sumsad = 0
json_file = open('s.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("s.h5")


emotion = []
c=0
while True:
    r,frame = camera.read()
    cv2.imshow("MOOD UP!",frame)
    c = c+1
    if c == 100:
        for i in range(n):
            ret,image = camera.read()
            cv2.imshow("MOOD UP!",image)
            img = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
            a.append(get_landmarks(img))
            
        b = np.array(a)   
        try:
            prediction = loaded_model.predict(b)
            for i in range(n):
                sumanger = sumanger+prediction[i][0]
                sumfear = sumfear+prediction[i][1]
                sumhappy = sumhappy+prediction[i][2]
                sumneutral = sumneutral+prediction[i][3]
                sumsad = sumsad+prediction[i][4]
            m = float(n)
            emotion.append(sumanger/m)
            emotion.append(sumfear/m)
            emotion.append(sumhappy/m)
            emotion.append(sumneutral/m)
            emotion.append(sumsad/m)
            break
        except:
            print ("Your face didn't get detected!")
            c=0
            a = []
            pass
    ch = cv2.waitKey(1)
    if ch & 0xFF == ord('q'):
        break
                
camera.release()
cv2.destroyAllWindows()


max1 = 0
for i in range(5):
    if emotion[i] > max1:
        max1 = emotion[i]
        b = i
if emotion[4] > 0.11:
    b = 4
if (emotion[3] > 0.10) and (emotion[4] > 0.11):
    b = 3
if (emotion[4] > 0.11) and (emotion[1]>0.27):
    b = 1
if (emotion[4] > 0.11) and (emotion[0]>0.34):
    b = 0
if b == 0:
    anger()
if b == 1:
    fear()
if b == 2:
    happy()
if b == 3:
    neutral()
if b == 4:
    sad()
            
            






note.this gives the percentage values of every emotion
STEP 6:
Our fun application
All the above effort of recognising the emotion for what? There has to be some implementation. Well there are many. When we thought of how we could use our very own trained model that recognises emotions we came across the following ideas.    
1. Making Cars Safer and Personalized
2. Facial Emotion Detection in Interviews 
3.  Testing for Video Games
4. Market Research. 
5. In banks for security purposes -detection of nervous people with ill intentions                        
However we wanted to make something very interesting and something that could be used at this stage of college. So we made a MOOD IMPROVISER!  Our app has the ability to do 4 things to manipulate and improve your mood.            
1. Play appropriate and soothing songs.  
2. Memes for some humour because why not!
3. Inspiring and happy posts because a little praise is all we need.
4. Happy YouTube video suggestions
The below code is basically 5 functions made (1 for each emotion ) and called when the corresponding emotion is detected to perform the above tasks!
App Code: 
from tkinter import *
from tkinter import ttk
import webbrowser as wb
from pygame import *
import cv2
import random as rd
import os
import pandas as pd
from text import happy_text,anger_text,pickup_text,fear_text,neutral_text,sad_text


data1 = pd.read_csv('youtube urls1.csv')
urls = data1.iloc[0:29,0]
button_names = data1.iloc[0:29,1]


def callback(url):
    mixer.music.stop()
    wb.get("C:/Program Files (x86)/Google/Chrome/Application/chrome.exe %s").open(url)


def quit_window(r):
    r.destroy()
    mixer.music.stop()


def happy():
    path1 = "songs/happy"
    music = os.listdir(path1)
    mixer.init()
    a = rd.randrange(len(music))
    mixer.music.load(path1 + '//'+ music[a])
    mixer.music.play()
    root = Tk()
    rd_array = rd.sample(range(64),8)
    path = "ppm_resized"
    pics = os.listdir(path)
    while True:
        root.title("MOOD UP !")
        root.resizable(False,False)
        root.configure(background = "#ffffd0")
        style = ttk.Style()
        style.theme_use('alt')
        style.configure('TButton',background = "#fee03b",foreground = "#ffffff",
                        font = ("Kristen ITC", 18),wraplength = 225,
                        justify = CENTER)
        style.configure('1.TButton',background = "#ff3367",foreground = "#ffffff",
                        font = ("Kristen ITC", 18),wraplength = 225,
                        justify = CENTER)
        style.configure('TFrame',background = "#ffffd0")
        style.configure('TLabel',foreground = "#ffffff",background = "#13254b")
        logo = PhotoImage(file = path + '//' +pics[rd_array[0]])
        label = ttk.Label(root,image = logo)
        label.grid(row = 0,column = 0)
        logo1 = PhotoImage(file = path + '//' +pics[rd_array[1]])
        label1 = ttk.Label(root,image = logo1)
        label1.grid(row = 0,column =1)
        logo2 = PhotoImage(file = path + '//' +pics[rd_array[2]])
        label2 = ttk.Label(root,image = logo2)
        label2.grid(row = 0,column = 2)
        logo3 = PhotoImage(file = path + '//' +pics[rd_array[3]])
        label3 = ttk.Label(root,image = logo3)
        label3.grid(row = 1,column = 0)
        logo4 = PhotoImage(file = path + '//' +pics[rd_array[4]])
        label4 = ttk.Label(root,image = logo4)
        label4.grid(row = 1,column = 2)
        logo5 = PhotoImage(file = path + '//' +pics[rd_array[5]])
        label5 = ttk.Label(root,image = logo5)
        label5.grid(row = 2,column = 0)
        logo6 = PhotoImage(file = path + '//' +pics[rd_array[6]])
        label6 = ttk.Label(root,image = logo6)
        label6.grid(row = 2,column = 1)
        logo7 = PhotoImage(file = path + '//' +pics[rd_array[7]])
        label7 = ttk.Label(root,image = logo7)
        label7.grid(row = 2,column = 2)
        text1 = rd.randrange(len(happy_text))
        label8 = ttk.Label(root, text = happy_text[text1],justify = CENTER,font = ("Kristen ITC", 18),wraplength = 225)
        label8.grid(row = 1, column =1)
        frame = ttk.Frame(root)
        frame.grid(row = 0,column = 3,rowspan = 3)
        frame.config(height = 675,width = 225)
        l = ttk.Label(frame,text = pickup_text[0],justify = CENTER,font = ("Kristen ITC", 18),wraplength = 500)
        l.grid(row = 0,column=0)
        




        button1 = ttk.Button(frame,text = button_names[0]
                             ,command = lambda: callback(urls[0]),width = 25)
        button1.grid(row = 1,column = 0)
        
        button2 = ttk.Button(frame,text = button_names[1]
                             ,command = lambda: callback(urls[1]),width = 25,style = '1.TButton')
        button2.grid(row = 2,column = 0)
        
        button3 = ttk.Button(frame,text = button_names[2]
                             ,command = lambda: callback(urls[2]),width = 25)
        button3.grid(row = 3,column = 0)
        
        button4 = ttk.Button(frame,text = button_names[3]
                             ,command = lambda: callback(urls[3]),width = 25,style = '1.TButton')
        button4.grid(row = 4,column = 0)
        
        button5 = ttk.Button(frame,text = button_names[4]
                             ,command = lambda: callback(urls[4]),width = 25)
        button5.grid(row = 5,column = 0)
        button5 = ttk.Button(frame,text = "Quit"
                             ,command = lambda: quit_window(root),width = 25,style = '1.TButton')
        button5.grid(row = 6,column = 0)
        root.mainloop()
  
def anger():
    path1 = "songs/anger"
    music = os.listdir(path1)
    mixer.init()
    a = rd.randrange(len(music))
    mixer.music.load(path1 + '//'+ music[a])
    mixer.music.play()
    root = Tk()
    rd_array = rd.sample(range(64),8)
    path = "ppm_resized"
    pics = os.listdir(path)
    while True:
        root.title("MOOD UP !")
        root.resizable(False,False)
        root.configure(background = "#fff3f3")
        style = ttk.Style()
        style.theme_use('alt')
        style.configure('TButton',background = "#f03967",foreground = "#ffffff",
                        font = ("Pristina", 20),wraplength = 225,
                        justify = CENTER)
        style.configure('1.TButton',background = "#bfaa99",foreground = "#ffffff",
                        font = ("Pristina", 20),wraplength = 225,
                        justify = CENTER)
        style.configure('TFrame',background = "#fff3f3")
        style.configure('TLabel',foreground = "#ffffff",background = "#7a56b4")
        logo = PhotoImage(file = path + '//' +pics[rd_array[0]])
        label = ttk.Label(root,image = logo)
        label.grid(row = 0,column = 0)
        logo1 = PhotoImage(file = path + '//' +pics[rd_array[1]])
        label1 = ttk.Label(root,image = logo1)
        label1.grid(row = 0,column =1)
        logo2 = PhotoImage(file = path + '//' +pics[rd_array[2]])
        label2 = ttk.Label(root,image = logo2)
        label2.grid(row = 0,column = 2)
        logo3 = PhotoImage(file = path + '//' +pics[rd_array[3]])
        label3 = ttk.Label(root,image = logo3)
        label3.grid(row = 1,column = 0)
        logo4 = PhotoImage(file = path + '//' +pics[rd_array[4]])
        label4 = ttk.Label(root,image = logo4)
        label4.grid(row = 1,column = 2)
        logo5 = PhotoImage(file = path + '//' +pics[rd_array[5]])
        label5 = ttk.Label(root,image = logo5)
        label5.grid(row = 2,column = 0)
        logo6 = PhotoImage(file = path + '//' +pics[rd_array[6]])
        label6 = ttk.Label(root,image = logo6)
        label6.grid(row = 2,column = 1)
        logo7 = PhotoImage(file = path + '//' +pics[rd_array[7]])
        label7 = ttk.Label(root,image = logo7)
        label7.grid(row = 2,column = 2)
        text1 = rd.randrange(len(anger_text))
        label8 = ttk.Label(root, text = anger_text[text1],justify = CENTER,font = ("Pristina", 20),wraplength = 225)
        label8.grid(row = 1, column =1)
        frame = ttk.Frame(root)
        frame.grid(row = 0,column = 3,rowspan = 3)
        frame.config(height = 675,width = 225)
        l = ttk.Label(frame,text = pickup_text[1],justify = CENTER,font = ("Pristina", 20),wraplength = 500)
        l.grid(row = 0,column=0)
        
 
        button1 = ttk.Button(frame,text = button_names[6]
                             ,command = lambda: callback(urls[6]),width = 25,style = '1.TButton')
        button1.grid(row = 1,column = 0)
        
        button2 = ttk.Button(frame,text = button_names[7]
                             ,command = lambda: callback(urls[7]),width = 25)
        button2.grid(row = 2,column = 0)
        
        button3 = ttk.Button(frame,text = button_names[8]
                             ,command = lambda: callback(urls[8]),width = 25,style = '1.TButton')
        button3.grid(row = 3,column = 0)
        
        button4 = ttk.Button(frame,text = button_names[9]
                             ,command = lambda: callback(urls[9]),width = 25)
        button4.grid(row = 4,column = 0)
        
        button5 = ttk.Button(frame,text = button_names[10]
                             ,command = lambda: callback(urls[10]),width = 25,style = '1.TButton')
        button5.grid(row = 5,column = 0)
        button5 = ttk.Button(frame,text = "Quit"
                             ,command = lambda: quit_window(root),width = 25)
        button5.grid(row = 6,column = 0)
        root.mainloop()
def sad():
    path1 = "songs/sad"
    music = os.listdir(path1)
    mixer.init()
    a = rd.randrange(len(music))
    mixer.music.load(path1 + '//'+ music[a])
    mixer.music.play()
    root = Tk()
    rd_array = rd.sample(range(64),8)
    path = "ppm_resized"
    pics = os.listdir(path)
    while True:
        root.title("MOOD UP !")
        root.resizable(False,False)
        root.configure(background = "#f5fcf8")
        style = ttk.Style()
        style.theme_use('alt')
        style.configure('TButton',background = "#fff73f",foreground = "#17113e",
                        font = ("Maiandra GD", 18,"bold"),wraplength = 225,
                        justify = CENTER)
        style.configure('1.TButton',background = "#84d4f6",foreground = "#f2e4b8",
                        font = ("Maiandra GD", 18,"bold"),wraplength = 225,
                        justify = CENTER)
        style.configure('TFrame',background = "#f5fcf8")
        style.configure('TLabel',foreground = "#ffffff",background = "#0d074d")
        logo = PhotoImage(file = path + '//' +pics[rd_array[0]])
        label = ttk.Label(root,image = logo)
        label.grid(row = 0,column = 0)
        logo1 = PhotoImage(file = path + '//' +pics[rd_array[1]])
        label1 = ttk.Label(root,image = logo1)
        label1.grid(row = 0,column =1)
        logo2 = PhotoImage(file = path + '//' +pics[rd_array[2]])
        label2 = ttk.Label(root,image = logo2)
        label2.grid(row = 0,column = 2)
        logo3 = PhotoImage(file = path + '//' +pics[rd_array[3]])
        label3 = ttk.Label(root,image = logo3)
        label3.grid(row = 1,column = 0)
        logo4 = PhotoImage(file = path + '//' +pics[rd_array[4]])
        label4 = ttk.Label(root,image = logo4)
        label4.grid(row = 1,column = 2)
        logo5 = PhotoImage(file = path + '//' +pics[rd_array[5]])
        label5 = ttk.Label(root,image = logo5)
        label5.grid(row = 2,column = 0)
        logo6 = PhotoImage(file = path + '//' +pics[rd_array[6]])
        label6 = ttk.Label(root,image = logo6)
        label6.grid(row = 2,column = 1)
        logo7 = PhotoImage(file = path + '//' +pics[rd_array[7]])
        label7 = ttk.Label(root,image = logo7)
        label7.grid(row = 2,column = 2)
        text1 = rd.randrange(len(sad_text))
        label8 = ttk.Label(root, text = sad_text[text1],justify = CENTER,font = ("Maiandra GD",18,"bold"),wraplength = 225)
        label8.grid(row = 1, column =1)
        frame = ttk.Frame(root)
        frame.grid(row = 0,column = 3,rowspan = 3)
        frame.config(height = 675,width = 225)
        l = ttk.Label(frame,text = pickup_text[2],justify = CENTER,font = ("Maiandra GD",18,"bold"),wraplength = 500)
        l.grid(row = 0,column=0)
        
        button1 = ttk.Button(frame,text = button_names[12]
                             ,command = lambda: callback(urls[12]),width = 25,style = '1.TButton')
        button1.grid(row = 1,column = 0)
        
        button2 = ttk.Button(frame,text = button_names[13]
                             ,command = lambda: callback(urls[13]),width = 25)
        button2.grid(row = 2,column = 0)
        
        button3 = ttk.Button(frame,text = button_names[14]
                             ,command = lambda: callback(urls[14]),width = 25,style = '1.TButton')
        button3.grid(row = 3,column = 0)
        
        button4 = ttk.Button(frame,text = button_names[15]
                             ,command = lambda: callback(urls[15]),width = 25)
        button4.grid(row = 4,column = 0)
        
        button5 = ttk.Button(frame,text = button_names[16]
                             ,command = lambda: callback(urls[16]),width = 25,style = '1.TButton')
        button5.grid(row = 5,column = 0)
        button5 = ttk.Button(frame,text = "Quit"
                             ,command = lambda: quit_window(root),width = 25)
        button5.grid(row = 6,column = 0)
        root.mainloop()
def fear():
    path1 = "songs/fear"
    music = os.listdir(path1)
    mixer.init()
    a = rd.randrange(len(music))
    mixer.music.load(path1 + '//'+ music[a])
    mixer.music.play()
    root = Tk()
    rd_array = rd.sample(range(64),8)
    path = "ppm_resized"
    pics = os.listdir(path)
    while True:
        root.title("MOOD UP !")
        root.resizable(False,False)
        root.configure(background = "#fff3f3")
        style = ttk.Style()
        style.theme_use('alt')
        style.configure('TButton',background = "#deafe1",foreground = "#5c599c",
                        font = ("Comic Sans MS", 18),wraplength = 225,
                        justify = CENTER)
        style.configure('1.TButton',background = "#ff1b50",foreground = "#2c0d21",
                        font = ("Comic Sans MS", 18),wraplength = 225,
                        justify = CENTER)
        style.configure('TFrame',background = "#fff3f3")
        style.configure('TLabel',foreground = "#ffffff",background = "#0d1246")
        logo = PhotoImage(file = path + '//' +pics[rd_array[0]])
        label = ttk.Label(root,image = logo)
        label.grid(row = 0,column = 0)
        logo1 = PhotoImage(file = path + '//' +pics[rd_array[1]])
        label1 = ttk.Label(root,image = logo1)
        label1.grid(row = 0,column =1)
        logo2 = PhotoImage(file = path + '//' +pics[rd_array[2]])
        label2 = ttk.Label(root,image = logo2)
        label2.grid(row = 0,column = 2)
        logo3 = PhotoImage(file = path + '//' +pics[rd_array[3]])
        label3 = ttk.Label(root,image = logo3)
        label3.grid(row = 1,column = 0)
        logo4 = PhotoImage(file = path + '//' +pics[rd_array[4]])
        label4 = ttk.Label(root,image = logo4)
        label4.grid(row = 1,column = 2)
        logo5 = PhotoImage(file = path + '//' +pics[rd_array[5]])
        label5 = ttk.Label(root,image = logo5)
        label5.grid(row = 2,column = 0)
        logo6 = PhotoImage(file = path + '//' +pics[rd_array[6]])
        label6 = ttk.Label(root,image = logo6)
        label6.grid(row = 2,column = 1)
        logo7 = PhotoImage(file = path + '//' +pics[rd_array[7]])
        label7 = ttk.Label(root,image = logo7)
        label7.grid(row = 2,column = 2)
        text1 = rd.randrange(len(fear_text))
        label8 = ttk.Label(root, text = fear_text[text1],justify = CENTER,font = ("Comic Sans MS", 18),wraplength = 225)
        label8.grid(row = 1, column =1)
        frame = ttk.Frame(root)
        frame.grid(row = 0,column = 3,rowspan = 3)
        frame.config(height = 675,width = 225)
        l = ttk.Label(frame,text = pickup_text[3],justify = CENTER,font = ("Comic Sans MS", 18),wraplength = 500)
        l.grid(row = 0,column=0)
        
        button1 = ttk.Button(frame,text = button_names[18]
                             ,command = lambda: callback(urls[18]),width = 25,style = '1.TButton')
        button1.grid(row = 1,column = 0)
        
        button2 = ttk.Button(frame,text = button_names[19]
                             ,command = lambda: callback(urls[19]),width = 25)
        button2.grid(row = 2,column = 0)
        
        button3 = ttk.Button(frame,text = button_names[20]
                             ,command = lambda: callback(urls[20]),width = 25,style = '1.TButton')
        button3.grid(row = 3,column = 0)
        
        button4 = ttk.Button(frame,text = button_names[21]
                             ,command = lambda: callback(urls[21]),width = 25)
        button4.grid(row = 4,column = 0)
        
        button5 = ttk.Button(frame,text = button_names[22]
                             ,command = lambda: callback(urls[22]),width = 25,style = '1.TButton')
        button5.grid(row = 5,column = 0)
        button5 = ttk.Button(frame,text = "Quit"
                             ,command = lambda: quit_window(root),width = 25)
        button5.grid(row = 6,column = 0)
        root.mainloop()
def neutral():
    path1 = "songs/neutral"
    music = os.listdir(path1)
    mixer.init()
    a = rd.randrange(len(music))
    mixer.music.load(path1 + '//'+ music[a])
    mixer.music.play()
    root = Tk()
    rd_array = rd.sample(range(64),8)
    path = "ppm_resized"
    pics = os.listdir(path)
    while True:
        root.title("MOOD UP !")
        root.resizable(False,False)
        root.configure(background = "#fff7cc")
        style = ttk.Style()
        style.theme_use('alt')
        style.configure('TButton',background = "#214fc6",foreground = "#ffffff",#51ffd8
                        font = ("Bradley Hand ITC", 18,"bold"),wraplength = 225,
                        justify = CENTER)
        style.configure('1.TButton',background = "#17fdad",foreground = "#724500",
                        font = ("Bradley Hand ITC", 18,"bold"),wraplength = 225,
                        justify = CENTER)
        style.configure('TFrame',background = "#fff7cc")
        style.configure('TLabel',foreground = "#724500",background = "#fffc00")
        logo = PhotoImage(file = path + '//' +pics[rd_array[0]])
        label = ttk.Label(root,image = logo)
        label.grid(row = 0,column = 0)
        logo1 = PhotoImage(file = path + '//' +pics[rd_array[1]])
        label1 = ttk.Label(root,image = logo1)
        label1.grid(row = 0,column =1)
        logo2 = PhotoImage(file = path + '//' +pics[rd_array[2]])
        label2 = ttk.Label(root,image = logo2)
        label2.grid(row = 0,column = 2)
        logo3 = PhotoImage(file = path + '//' +pics[rd_array[3]])
        label3 = ttk.Label(root,image = logo3)
        label3.grid(row = 1,column = 0)
        logo4 = PhotoImage(file = path + '//' +pics[rd_array[4]])
        label4 = ttk.Label(root,image = logo4)
        label4.grid(row = 1,column = 2)
        logo5 = PhotoImage(file = path + '//' +pics[rd_array[5]])
        label5 = ttk.Label(root,image = logo5)
        label5.grid(row = 2,column = 0)
        logo6 = PhotoImage(file = path + '//' +pics[rd_array[6]])
        label6 = ttk.Label(root,image = logo6)
        label6.grid(row = 2,column = 1)
        logo7 = PhotoImage(file = path + '//' +pics[rd_array[7]])
        label7 = ttk.Label(root,image = logo7)
        label7.grid(row = 2,column = 2)
        text1 = rd.randrange(len(neutral_text))
        label8 = ttk.Label(root, text = neutral_text[text1],justify = CENTER,font = ("Bradley Hand ITC", 18,"bold"),wraplength = 225)
        label8.grid(row = 1, column =1)
        frame = ttk.Frame(root)
        frame.grid(row = 0,column = 3,rowspan = 3)
        frame.config(height = 675,width = 225)
        l = ttk.Label(frame,text = pickup_text[4],justify = CENTER,font = ("Bradley Hand ITC", 18,"bold"),wraplength = 500)
        l.grid(row = 0,column=0)
        
        button1 = ttk.Button(frame,text = button_names[24]
                             ,command = lambda: callback(urls[24]),width = 25,style = '1.TButton')
        button1.grid(row = 1,column = 0)
        
        button2 = ttk.Button(frame,text = button_names[25]
                             ,command = lambda: callback(urls[25]),width = 25)
        button2.grid(row = 2,column = 0)
        
        button3 = ttk.Button(frame,text = button_names[26]
                             ,command = lambda: callback(urls[26]),width = 25,style = '1.TButton')
        button3.grid(row = 3,column = 0)
        
        button4 = ttk.Button(frame,text = button_names[27]
                             ,command = lambda: callback(urls[27]),width = 25)
        button4.grid(row = 4,column = 0)
        
        button5 = ttk.Button(frame,text = button_names[28]
                             ,command = lambda: callback(urls[28]),width = 25,style = '1.TButton')
        button5.grid(row = 5,column = 0)
        button5 = ttk.Button(frame,text = "Quit"
                             ,command = lambda: quit_window(root),width = 25)
        button5.grid(row = 6,column = 0)
        root.mainloop()
Note: Text and youtube urls are loaded from a python and csv file respectively while songs are loaded from a normal file.